{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with PySpark\n",
    "\n",
    "Apache Spark is an open-source cluster-computing framework. Spark provides an interface for programming entire clusters with implicit data parallelism and fault-tolerance. [Wikipedia](https://en.wikipedia.org/wiki/Apache_Spark)\n",
    "\n",
    "While Spark is writen in Scala, a language that compiles down to bytecode for the JVM, the open source community has developed a wonderful toolkit called PySpark that allows you to interface with RDD's in Python. Thanks to a library called Py4J, Python can interface with JVM objects, in our case RDD's, and this library one of the tools that makes PySpark work. [Read more about PySpark here](http://www.kdnuggets.com/2015/11/introduction-spark-python.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by installing and running PySpark on our local machine.  \n",
    "\n",
    "1. Open a command line separate from the one on which you're running Jupyter.   \n",
    "2. Install on command line using \"pip install pyspark\"    \n",
    "\n",
    "You may need to restart your terminal to be able to run PySpark. (wasn't required running on a mac) \n",
    "\n",
    "3. To check that pyspark is working, you can start pyspark directly on your command line, by typing \"pyspark\"  You should see:  \n",
    "\n",
    "Welcome to  \n",
    "       ____              __  \n",
    "      / __/__  ___ _____/ /__  \n",
    "     _\\ \\/ _ \\/ _ `/ __/  '_/  \n",
    "    /__ / .__/\\_,_/_/ /_/\\_\\   version 2.2.0  \n",
    "       /_/  \n",
    "  \n",
    "4. You can quit the command line PySpark by typing \"quit()\".  \n",
    "\n",
    "5. Next, add the pyspark-shell to the shell environment variable   PYSPARK_SUBMIT_ARGS, by typing:  \n",
    "export PYSPARK_SUBMIT_ARGS=\"--master local[2] pyspark-shell  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc =SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14198288\n"
     ]
    }
   ],
   "source": [
    "# Calculate a very special number using PySpark!\n",
    "import random\n",
    "num_samples = 100000000\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## PySpark in Jupyter  \n",
    "If the above procedure didn't work, there are two other ways to get PySpark available in a Jupyter Notebook:\n",
    "\n",
    "1. Configure PySpark driver to use Jupyter Notebook: running pyspark will automatically open a Jupyter Notebook  \n",
    "\n",
    "2. Load a regular Jupyter Notebook and load PySpark using findSpark package  \n",
    "\n",
    "First option is quicker but specific to Jupyter Notebook, second option is a broader approach to get PySpark available in your favorite IDE.  \n",
    "\n",
    "[Source](https://blog.sicara.com/get-started-pyspark-jupyter-guide-tutorial-ae2fe84f594f)\n",
    "\n",
    "### Method 1 — Configure PySpark driver  \n",
    "\n",
    "Update PySpark driver environment variables: add these lines to your ~/.bashrc (or ~/.zshrc) file. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the command line, enter\n",
    "\n",
    "export PYSPARK_DRIVER_PYTHON=jupyter\n",
    "export PYSPARK_DRIVER_PYTHON_OPTS='notebook'  \n",
    "\n",
    "Restart your terminal and launch PySpark again:  \n",
    "\n",
    "$ pyspark  \n",
    "\n",
    "Now, this command should start a Jupyter Notebook in your web browser. Create a new notebook by clicking on ‘New’ > ‘Notebooks Python [default]’.\n",
    "Copy and paste our Pi calculation script and run it by pressing Shift + Enter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Method 2 — FindSpark package\n",
    "There is another and more generalized way to use PySpark in a Jupyter Notebook: use findSpark package to make a Spark Context available in your code.  \n",
    "\n",
    "findSpark package is not specific to Jupyter Notebook, you can use this trick in your favorite IDE too.  \n",
    "\n",
    "To install findspark:  \n",
    "\n",
    "$ pip install findspark\n",
    "\n",
    "Launch a regular Jupyter Notebook:  \n",
    "\n",
    "$ jupyter notebook  \n",
    "\n",
    "Create a new Python [default] notebook and write the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The only difference from above are the first two lines initializing findspark.\n",
    "\n",
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import random\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 100000000\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Let's do a bit more with pyspark\n",
    "\n",
    "Here, we count characters\n",
    "[Source](https://github.com/KristianHolsheimer/pyspark-setup-guide/blob/master/spark_word_count.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is just to add a sound that can alert us when a cell finishes executing.\n",
    "from IPython.display import Audio\n",
    "sound_file = './../beep.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can't run two instances at the same time, so you'll need to quit the previous instance first.\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-07-26 19:21:30--  http://www.gutenberg.org/cache/epub/100/pg100.txt\n",
      "Resolving www.gutenberg.org... 152.19.134.47, 2610:28:3090:3000::bad:cafe:47\n",
      "Connecting to www.gutenberg.org|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5589889 (5.3M) [text/plain]\n",
      "Saving to: ‘pg100.txt.3’\n",
      "\n",
      "pg100.txt.3         100%[===================>]   5.33M  3.35MB/s    in 1.6s    \n",
      "\n",
      "2017-07-26 19:21:32 (3.35 MB/s) - ‘pg100.txt.3’ saved [5589889/5589889]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"./beep.wav\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wget 'http://www.gutenberg.org/cache/epub/100/pg100.txt'\n",
    "Audio(url=sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_text = sc.textFile('pg100.txt', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first line of raw_text:\t \"The Project Gutenberg EBook of The Complete Works of William Shakespeare, by\"\n",
      "total number of lines:\t 124787\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"/*code/beep.wav\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check whether the data was loaded properly:\n",
    "print( u'first line of raw_text:\\t \"{}\"'.format(raw_text.first()))\n",
    "print( u'total number of lines:\\t {}'.format(raw_text.count()))\n",
    "Audio(url=sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
